# ğŸ¯ Option C: Cáº£i thiá»‡n Há»‡ thá»‘ng Hiá»‡n táº¡i

## Quick Wins - KhÃ´ng cáº§n rebuild architecture

---

## âœ… Step 1: Fix LONGTEXT Migration (NGAY BÃ‚Y GIá»œ)

### 1.1. Cháº¡y migration:

```bash
cd backend
npx prisma migrate dev --name change_text_to_longtext
```

Hoáº·c náº¿u npx khÃ´ng work:
```bash
cd backend
npm run prisma:migrate
# Enter name: change_text_to_longtext
```

### 1.2. Generate Prisma Client:

```bash
npm run prisma:generate
```

### 1.3. Restart backend:

```bash
npm run start:dev
```

**Test:** Upload má»™t file tÃ i liá»‡u (~50-100KB) Ä‘á»ƒ verify lá»—i Ä‘Ã£ fix.

---

## ğŸ” Step 2: Add Hybrid Search (Full-text + Semantic)

### 2.1. Add MySQL FULLTEXT Index

Táº¡o migration má»›i:

```bash
cd backend
npx prisma migrate dev --create-only --name add_fulltext_search
```

Edit file migration vá»«a táº¡o (trong `prisma/migrations/...`):

```sql
-- Add FULLTEXT index to Document.content
ALTER TABLE `documents` ADD FULLTEXT INDEX `idx_document_content` (`content`);

-- Add FULLTEXT index to Question.content
ALTER TABLE `questions` ADD FULLTEXT INDEX `idx_question_content` (`content`);
```

Apply migration:

```bash
npx prisma migrate dev
```

### 2.2. Update DocumentsService vá»›i Hybrid Search

Create: `backend/src/documents/search.service.ts`

```typescript
import { Injectable } from '@nestjs/common';
import { PrismaService } from '../prisma/prisma.service';
import { AiService } from '../ai/ai.service';

interface SearchResult {
  id: string;
  content: string;
  score: number;
  source: 'fulltext' | 'semantic';
}

@Injectable()
export class SearchService {
  constructor(
    private prisma: PrismaService,
    private aiService: AiService,
  ) {}

  /**
   * Detect if query needs exact matching
   */
  private isExactQuery(query: string): boolean {
    // Has quotes "..."
    if (query.includes('"')) return true;
    
    // Has math formula patterns
    const mathPatterns = [
      /\d+\s*[\+\-\*\/\=]\s*\d+/,  // 2 + 3, x = 5
      /[a-z]\^?\d/i,                 // x^2, a2
      /[âˆšâˆ«âˆ‘âˆ]/,                      // Math symbols
    ];
    if (mathPatterns.some(pattern => pattern.test(query))) return true;
    
    // Has exact phrase indicators
    const exactIndicators = ['chÃ­nh xÃ¡c', 'Ä‘Ãºng lÃ ', 'cÃ´ng thá»©c'];
    if (exactIndicators.some(ind => query.toLowerCase().includes(ind))) return true;
    
    return false;
  }

  /**
   * Full-text search using MySQL MATCH AGAINST
   */
  private async fullTextSearch(
    query: string,
    subjectId: string,
    limit: number = 10,
  ): Promise<SearchResult[]> {
    // Use raw SQL for FULLTEXT search
    const results = await this.prisma.$queryRaw<any[]>`
      SELECT 
        id,
        content,
        originalFileName as source,
        MATCH(content) AGAINST(${query} IN NATURAL LANGUAGE MODE) as score
      FROM documents
      WHERE 
        subjectId = ${subjectId}
        AND MATCH(content) AGAINST(${query} IN NATURAL LANGUAGE MODE)
      ORDER BY score DESC
      LIMIT ${limit}
    `;

    return results.map(r => ({
      id: r.id,
      content: r.content,
      score: r.score,
      source: 'fulltext' as const,
    }));
  }

  /**
   * Semantic search using embeddings
   */
  private async semanticSearch(
    query: string,
    subjectId: string,
    limit: number = 10,
  ): Promise<SearchResult[]> {
    // Generate query embedding
    const queryEmbedding = await this.aiService.generateEmbedding(query);

    // Get all documents with embeddings
    const documents = await this.prisma.document.findMany({
      where: {
        subjectId,
        embedding: { not: null },
      },
    });

    // Calculate similarity
    const scored = documents
      .map(doc => {
        if (!doc.embedding) return null;
        const embedding = doc.embedding as number[];
        const similarity = this.cosineSimilarity(queryEmbedding, embedding);
        return {
          id: doc.id,
          content: doc.content || '',
          score: similarity,
          source: 'semantic' as const,
        };
      })
      .filter(item => item !== null && item.score > 0.7)
      .sort((a, b) => b.score - a.score)
      .slice(0, limit);

    return scored;
  }

  /**
   * Cosine similarity
   */
  private cosineSimilarity(a: number[], b: number[]): number {
    if (a.length !== b.length) return 0;
    let dotProduct = 0;
    let normA = 0;
    let normB = 0;
    for (let i = 0; i < a.length; i++) {
      dotProduct += a[i] * b[i];
      normA += a[i] * a[i];
      normB += b[i] * b[i];
    }
    return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
  }

  /**
   * Hybrid search: Combines full-text and semantic search
   */
  async hybridSearch(
    query: string,
    subjectId: string,
    limit: number = 10,
  ): Promise<SearchResult[]> {
    const isExact = this.isExactQuery(query);

    console.log(`ğŸ” Query: "${query}" | Mode: ${isExact ? 'EXACT' : 'SEMANTIC'}`);

    if (isExact) {
      // Use full-text search for exact queries
      return this.fullTextSearch(query, subjectId, limit);
    } else {
      // Use semantic search for conceptual queries
      return this.semanticSearch(query, subjectId, limit);
    }
  }

  /**
   * Advanced: Combine both and merge results
   */
  async combinedSearch(
    query: string,
    subjectId: string,
    limit: number = 10,
  ): Promise<SearchResult[]> {
    // Run both searches in parallel
    const [fullTextResults, semanticResults] = await Promise.all([
      this.fullTextSearch(query, subjectId, Math.ceil(limit / 2)),
      this.semanticSearch(query, subjectId, Math.ceil(limit / 2)),
    ]);

    // Merge and deduplicate
    const merged = new Map<string, SearchResult>();

    fullTextResults.forEach(r => {
      merged.set(r.id, { ...r, score: r.score * 0.6 }); // Weight full-text lower
    });

    semanticResults.forEach(r => {
      if (merged.has(r.id)) {
        // Boost score if found in both
        const existing = merged.get(r.id)!;
        merged.set(r.id, {
          ...r,
          score: existing.score + r.score * 0.4,
          source: 'fulltext' as const, // Mark as combined
        });
      } else {
        merged.set(r.id, { ...r, score: r.score * 0.4 });
      }
    });

    // Sort by score
    return Array.from(merged.values())
      .sort((a, b) => b.score - a.score)
      .slice(0, limit);
  }
}
```

### 2.3. Update DocumentsModule

```typescript
// backend/src/documents/documents.module.ts
import { Module } from '@nestjs/common';
import { DocumentsController } from './documents.controller';
import { DocumentsService } from './documents.service';
import { SearchService } from './search.service'; // NEW
import { PrismaModule } from '../prisma/prisma.module';
import { AiModule } from '../ai/ai.module';

@Module({
  imports: [PrismaModule, AiModule],
  controllers: [DocumentsController],
  providers: [DocumentsService, SearchService], // Add SearchService
  exports: [DocumentsService, SearchService],
})
export class DocumentsModule {}
```

### 2.4. Add Search Endpoint

```typescript
// backend/src/documents/documents.controller.ts
import { SearchService } from './search.service';

@Controller('documents')
export class DocumentsController {
  constructor(
    private readonly documentsService: DocumentsService,
    private readonly searchService: SearchService, // NEW
  ) {}

  // ... existing endpoints

  @Get('search')
  @ApiOperation({ summary: 'Hybrid search in documents' })
  async searchDocuments(
    @Query('query') query: string,
    @Query('subjectId') subjectId: string,
    @Query('limit') limit?: number,
  ) {
    return this.searchService.hybridSearch(
      query,
      subjectId,
      limit ? parseInt(limit.toString()) : 10,
    );
  }

  @Get('search/combined')
  @ApiOperation({ summary: 'Combined full-text + semantic search' })
  async combinedSearch(
    @Query('query') query: string,
    @Query('subjectId') subjectId: string,
    @Query('limit') limit?: number,
  ) {
    return this.searchService.combinedSearch(
      query,
      subjectId,
      limit ? parseInt(limit.toString()) : 10,
    );
  }
}
```

---

## ğŸ§© Step 3: Optimize Chunking Logic

### 3.1. Update DocumentsService - Smart Chunking

```typescript
// backend/src/documents/documents.service.ts

/**
 * Smart chunking - preserve paragraph structure
 */
private smartChunk(text: string): string[] {
  const chunks: string[] = [];
  const maxChunkSize = 3000; // characters
  const minChunkSize = 1000;
  const overlap = 400;

  // First, split by paragraphs (2+ newlines)
  const paragraphs = text.split(/\n\n+/).filter(p => p.trim().length > 0);

  let currentChunk = '';

  for (const paragraph of paragraphs) {
    const trimmedPara = paragraph.trim();

    // If single paragraph is too long, split it
    if (trimmedPara.length > maxChunkSize) {
      if (currentChunk.length > 0) {
        chunks.push(currentChunk);
        currentChunk = '';
      }

      // Split long paragraph by sentences
      const sentences = trimmedPara.split(/[.!?]\s+/);
      let subChunk = '';

      for (const sentence of sentences) {
        if ((subChunk + sentence).length > maxChunkSize) {
          if (subChunk.length > 0) {
            chunks.push(subChunk);
          }
          subChunk = sentence;
        } else {
          subChunk += (subChunk ? '. ' : '') + sentence;
        }
      }

      if (subChunk.length > 0) {
        chunks.push(subChunk);
      }

      continue;
    }

    // Try to add paragraph to current chunk
    if ((currentChunk + trimmedPara).length > maxChunkSize) {
      // Current chunk is full, save it
      if (currentChunk.length > 0) {
        chunks.push(currentChunk);
      }

      // Start new chunk with overlap
      if (currentChunk.length > overlap) {
        const overlapText = currentChunk.slice(-overlap);
        currentChunk = overlapText + '\n\n' + trimmedPara;
      } else {
        currentChunk = trimmedPara;
      }
    } else {
      // Add to current chunk
      currentChunk += (currentChunk ? '\n\n' : '') + trimmedPara;
    }
  }

  // Add remaining chunk
  if (currentChunk.length > 0) {
    chunks.push(currentChunk);
  }

  return chunks.filter(chunk => chunk.length >= minChunkSize);
}
```

Replace old chunking in `processEmbeddingsAsync`:

```typescript
// OLD:
// while (start < textContent.length && processedChunks < maxChunks) {
//   const end = Math.min(start + chunkSize, textContent.length);
//   const chunk = textContent.substring(start, end);
//   ...
// }

// NEW:
const chunks = this.smartChunk(textContent);
const maxChunks = 30;
const chunksToProcess = chunks.slice(0, maxChunks);

for (let i = 0; i < chunksToProcess.length; i++) {
  const chunk = chunksToProcess[i];
  // ... rest of processing
}
```

---

## ğŸ“ Step 4: Improve RAG Prompts

### 4.1. Update AiService - Better Prompts

```typescript
// backend/src/ai/ai.service.ts

async generateWithRAG(
  userId: string,
  actionType: ActionType,
  prompt: string,
  contextChunks: Array<{ content: string; source?: string }>,
  systemPrompt?: string,
): Promise<{ content: string; tokensUsed: number; confidence?: number }> {
  // ... existing quota check ...

  // IMPROVED: Build better context with structure
  const contextText = contextChunks
    .map((chunk, idx) => {
      const sourceInfo = chunk.source ? ` (Nguá»“n: ${chunk.source})` : '';
      return `ã€Äoáº¡n ${idx + 1}ã€‘${sourceInfo}\n${chunk.content}`;
    })
    .join('\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n');

  // IMPROVED: More specific system prompt
  const strictSystemPrompt = systemPrompt || `Báº¡n lÃ  trá»£ lÃ½ AI giÃºp giÃ¡o viÃªn THCS Viá»‡t Nam.

ğŸ“‹ QUY Táº®C NGHIÃŠM NGáº¶T - Báº®T BUá»˜C TUÃ‚N THá»¦:

1. âœ… CHá»ˆ sá»­ dá»¥ng thÃ´ng tin tá»« cÃ¡c Ä‘oáº¡n tÃ i liá»‡u Ä‘Æ°á»£c Ä‘Ã¡nh sá»‘ ã€Äoáº¡n 1ã€‘, ã€Äoáº¡n 2ã€‘... bÃªn dÆ°á»›i
2. âŒ TUYá»†T Äá»I KHÃ”NG sá»­ dá»¥ng kiáº¿n thá»©c bÃªn ngoÃ i, kinh nghiá»‡m cÃ¡ nhÃ¢n, hay thÃ´ng tin chung
3. ğŸ“ Má»—i cÃ¢u tráº£ lá»i PHáº¢I trÃ­ch dáº«n Ä‘oáº¡n cá»¥ thá»ƒ: "Theo ã€Äoáº¡n 2ã€‘, ..."
4. âš ï¸  Náº¿u thÃ´ng tin khÃ´ng Ä‘á»§ trong tÃ i liá»‡u â†’ NÃ“I RÃ•: "TÃ i liá»‡u khÃ´ng cung cáº¥p thÃ´ng tin vá»..."
5. ğŸ‡»ğŸ‡³ TuÃ¢n thá»§ chÆ°Æ¡ng trÃ¬nh SGK THCS Viá»‡t Nam (lá»›p 6-9)
6. ğŸ“Š Äáº£m báº£o Ä‘á»™ chÃ­nh xÃ¡c 100% - giÃ¡o dá»¥c khÃ´ng cháº¥p nháº­n sai sÃ³t
7. ğŸ¯ Output theo format yÃªu cáº§u (JSON náº¿u cáº§n structured data)

ğŸ’¡ Máº¸O: Khi táº¡o cÃ¢u há»i/Ä‘á» thi:
- CÃ¢u EASY: Kiáº¿n thá»©c cÆ¡ báº£n, nhá»› vÃ  hiá»ƒu
- CÃ¢u MEDIUM: Váº­n dá»¥ng trá»±c tiáº¿p cÃ´ng thá»©c/khÃ¡i niá»‡m
- CÃ¢u HARD: Tá»•ng há»£p nhiá»u kiáº¿n thá»©c, tÆ° duy pháº£n biá»‡n`;

  const fullPrompt = `${strictSystemPrompt}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“š TÃ€I LIá»†U NGUá»’N:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

${contextText}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â“ YÃŠU Cáº¦U Cá»¦A GIÃO VIÃŠN:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

${prompt}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âš ï¸ LÆ¯U Ã: Chá»‰ dÃ¹ng thÃ´ng tin tá»« cÃ¡c ã€Äoáº¡nã€‘ á»Ÿ trÃªn. Cite nguá»“n rÃµ rÃ ng.`;

  // ... rest of the function
}
```

### 4.2. Add Confidence Scoring

```typescript
/**
 * Calculate confidence based on context quality
 */
private calculateConfidence(
  contextChunks: Array<{ content: string; score?: number }>,
  response: string,
): number {
  let confidence = 0.5; // Base confidence

  // Factor 1: Number of relevant chunks
  if (contextChunks.length >= 5) confidence += 0.15;
  else if (contextChunks.length >= 3) confidence += 0.10;
  else if (contextChunks.length >= 1) confidence += 0.05;

  // Factor 2: Average relevance score
  const avgScore = contextChunks.reduce((sum, c) => sum + (c.score || 0.7), 0) / contextChunks.length;
  if (avgScore >= 0.85) confidence += 0.15;
  else if (avgScore >= 0.75) confidence += 0.10;

  // Factor 3: Response has citations
  const hasCitations = /ã€Äoáº¡n \d+ã€‘/.test(response);
  if (hasCitations) confidence += 0.10;

  // Factor 4: Response not too short (not "insufficient info")
  if (response.length > 200) confidence += 0.10;

  return Math.min(confidence, 1.0);
}

// Use in generateWithRAG:
const confidence = this.calculateConfidence(contextChunks, content);
return { content, tokensUsed, confidence };
```

---

## ğŸ“Š Step 5: Add Progress Tracking (Simple Version)

### 5.1. Add Status Field to Document (Already have)

Already in schema: `status: ProcessingStatus`

### 5.2. Update Frontend - Show Status

```typescript
// frontend/src/pages/Documents.tsx

// In document list rendering:
{documents.map((folder: any) => (
  <div key={folder.fileName} className="border border-gray-200 rounded-lg p-4">
    <div className="flex items-center justify-between mb-2">
      <div className="flex items-center gap-3">
        <Folder className="text-blue-600" size={24} />
        <div>
          <p className="font-medium text-gray-900">{folder.fileName}</p>
          <p className="text-sm text-gray-500">
            {folder.type} â€¢ {folder.chunks.length} chunks
          </p>
        </div>
      </div>
      
      {/* Status Badge */}
      <span className={`px-2 py-1 text-xs rounded-full ${
        folder.isProcessed 
          ? 'bg-green-100 text-green-700' 
          : 'bg-yellow-100 text-yellow-700 animate-pulse'
      }`}>
        {folder.isProcessed ? 'âœ“ ÄÃ£ xá»­ lÃ½' : 'â³ Äang xá»­ lÃ½...'}
      </span>
    </div>
    
    {/* Progress bar for processing */}
    {!folder.isProcessed && (
      <div className="mt-2">
        <div className="w-full bg-gray-200 rounded-full h-2">
          <div 
            className="bg-blue-600 h-2 rounded-full transition-all duration-500"
            style={{ 
              width: `${(folder.chunks.filter((c: any) => c.isProcessed).length / folder.chunks.length) * 100}%` 
            }}
          />
        </div>
        <p className="text-xs text-gray-500 mt-1">
          {folder.chunks.filter((c: any) => c.isProcessed).length} / {folder.chunks.length} chunks
        </p>
      </div>
    )}
  </div>
))}
```

### 5.3. Add Auto-refresh

```typescript
// frontend/src/pages/Documents.tsx

// Add polling for documents being processed
const { data: documentsData, isLoading } = useQuery({
  queryKey: ['documents', selectedSubject],
  queryFn: async () => {
    if (!selectedSubject) return { grouped: [], total: 0 };
    const res = await axios.get(`${API_URL}/documents/by-subject`, {
      params: { subjectId: selectedSubject },
    });
    return res.data;
  },
  enabled: !!selectedSubject,
  refetchInterval: (data) => {
    // Auto-refresh every 3s if any document is processing
    const hasProcessing = data?.some((d: any) => !d.isProcessed);
    return hasProcessing ? 3000 : false;
  },
});
```

---

## ğŸ¯ Step 6: Cache Query Embeddings (Optional - Future)

For popular queries, cache embeddings to save cost:

```typescript
// backend/src/ai/embedding-cache.service.ts
import { Injectable } from '@nestjs/common';

interface CachedEmbedding {
  query: string;
  embedding: number[];
  createdAt: Date;
  hits: number;
}

@Injectable()
export class EmbeddingCacheService {
  private cache: Map<string, CachedEmbedding> = new Map();
  private maxCacheSize = 100;

  async getOrGenerate(
    query: string,
    generator: () => Promise<number[]>,
  ): Promise<number[]> {
    const key = query.toLowerCase().trim();

    // Check cache
    if (this.cache.has(key)) {
      const cached = this.cache.get(key)!;
      cached.hits++;
      console.log(`ğŸ“¦ Cache HIT for query: "${query}" (${cached.hits} hits)`);
      return cached.embedding;
    }

    // Generate new
    console.log(`ğŸ”§ Cache MISS for query: "${query}", generating...`);
    const embedding = await generator();

    // Add to cache
    this.cache.set(key, {
      query: key,
      embedding,
      createdAt: new Date(),
      hits: 1,
    });

    // Evict old entries if cache too large
    if (this.cache.size > this.maxCacheSize) {
      const oldest = Array.from(this.cache.entries())
        .sort((a, b) => a[1].createdAt.getTime() - b[1].createdAt.getTime())[0];
      this.cache.delete(oldest[0]);
    }

    return embedding;
  }

  getCacheStats() {
    return {
      size: this.cache.size,
      queries: Array.from(this.cache.values())
        .sort((a, b) => b.hits - a.hits)
        .slice(0, 10)
        .map(c => ({ query: c.query, hits: c.hits })),
    };
  }
}
```

Use in SearchService:

```typescript
const queryEmbedding = await this.embeddingCache.getOrGenerate(
  query,
  () => this.aiService.generateEmbedding(query)
);
```

---

## âœ… Testing Checklist

After implementing each step:

- [ ] **Step 1 - LONGTEXT**: Upload 50-100KB file â†’ Should succeed
- [ ] **Step 2 - Hybrid Search**: 
  - Search "xÂ² + 2x + 1" â†’ Should use full-text
  - Search "cÃ¡ch giáº£i phÆ°Æ¡ng trÃ¬nh" â†’ Should use semantic
- [ ] **Step 3 - Smart Chunking**: Check chunks preserve paragraph boundaries
- [ ] **Step 4 - Better Prompts**: Generate exam â†’ Check for citations ã€Äoáº¡nã€‘
- [ ] **Step 5 - Progress**: Upload file â†’ See progress bar updating
- [ ] **Step 6 - Cache**: Same query twice â†’ Second should be faster

---

## ğŸ“ˆ Expected Improvements

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| Upload success rate | 70% | 95%+ | âœ… Fix LONGTEXT |
| Search accuracy | 75% | 85%+ | âœ… Hybrid search |
| Chunk quality | 60% | 80%+ | âœ… Smart chunking |
| User confidence | Low | High | âœ… Progress tracking |
| AI response quality | 70% | 85%+ | âœ… Better prompts |

---

## ğŸš€ Quick Start

```bash
# 1. Fix LONGTEXT
cd backend
npx prisma migrate dev --name change_text_to_longtext
npm run prisma:generate

# 2. Restart backend
npm run start:dev

# 3. Test upload
# Go to frontend â†’ Upload a document

# 4. Implement remaining steps gradually
# - Add search.service.ts
# - Update chunking
# - Improve prompts
# - Add progress UI
```

---

**Báº¯t Ä‘áº§u vá»›i Step 1 Ä‘á»ƒ fix lá»—i upload ngay!** ğŸ¯

