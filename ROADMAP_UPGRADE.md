# üöÄ EduGenie Teacher - Production Upgrade Roadmap

## üìã T·ªïng quan

Roadmap n√¢ng c·∫•p h·ªá th·ªëng t·ª´ MVP ‚Üí Production-Ready v·ªõi ki·∫øn tr√∫c scalable, RAG ch√≠nh x√°c, v√† UX t·ªët h∆°n.

---

## üéØ Core Improvements

### 1. ‚ö° Architecture: Microservices + Queue-based Processing

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      FRONTEND (React + PWA)                  ‚îÇ
‚îÇ  Upload Wizard ‚Üí Job Progress ‚Üí Preview ‚Üí Approve ‚Üí Export  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   NESTJS BACKEND API                        ‚îÇ
‚îÇ  ‚Ä¢ Authentication ‚Ä¢ Authorization ‚Ä¢ Business Logic          ‚îÇ
‚îÇ  ‚Ä¢ Queue Job Management ‚Ä¢ API Gateway                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ                             ‚îÇ
           ‚ñº                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   MESSAGE QUEUE      ‚îÇ    ‚îÇ    MYSQL DATABASE           ‚îÇ
‚îÇ   (BullMQ/RabbitMQ)  ‚îÇ    ‚îÇ  ‚Ä¢ User, Subscription       ‚îÇ
‚îÇ  ‚Ä¢ Document Jobs     ‚îÇ    ‚îÇ  ‚Ä¢ Document, Chunk          ‚îÇ
‚îÇ  ‚Ä¢ Embedding Jobs    ‚îÇ    ‚îÇ  ‚Ä¢ Question, Exam           ‚îÇ
‚îÇ  ‚Ä¢ Export Jobs       ‚îÇ    ‚îÇ  ‚Ä¢ LessonPlan, Usage        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ                              ‚îÇ
           ‚ñº                              ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ   PYTHON PROCESSING SERVICE          ‚îÇ  ‚îÇ
‚îÇ  ‚Ä¢ PDF/DOCX Parser (PyMuPDF)         ‚îÇ  ‚îÇ
‚îÇ  ‚Ä¢ Chapter Detection (regex/ML)      ‚îÇ  ‚îÇ
‚îÇ  ‚Ä¢ Smart Chunking (LangChain)        ‚îÇ‚óÑ‚îÄ‚îò
‚îÇ  ‚Ä¢ Embedding Generation (OpenAI)     ‚îÇ
‚îÇ  ‚Ä¢ Metadata Extraction               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      VECTOR DATABASE                 ‚îÇ
‚îÇ   (Qdrant / Pinecone / PGVector)     ‚îÇ
‚îÇ  ‚Ä¢ Fast similarity search            ‚îÇ
‚îÇ  ‚Ä¢ Filter by chapter/topic/grade     ‚îÇ
‚îÇ  ‚Ä¢ Metadata indexing                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìä Phase 1: Database Schema Upgrade (Week 1-2)

### Current Issues:
- ‚ùå Document ch·ª©a to√†n b·ªô content ‚Üí embedding kh√¥ng ch√≠nh x√°c
- ‚ùå Kh√¥ng c√≥ chapter/topic metadata ‚Üí kh√¥ng filter ƒë∆∞·ª£c
- ‚ùå Question kh√¥ng link source ‚Üí kh√¥ng trace ƒë∆∞·ª£c
- ‚ùå Exam kh√¥ng c√≥ scope ‚Üí t·∫°o ƒë·ªÅ kh√¥ng flexible

### New Schema:

```prisma
// ===== CORE ENTITIES =====

model Subject {
  id          String       @id @default(cuid())
  name        String       // To√°n, L√Ω, H√≥a...
  grade       Int          // 6, 7, 8, 9
  documents   Document[]
  questions   Question[]
  exams       Exam[]
  lessonPlans LessonPlan[]
  
  @@unique([name, grade])
  @@index([name, grade])
  @@map("subjects")
}

// ===== DOCUMENT & CHUNKS =====

model Document {
  id              String       @id @default(cuid())
  subjectId       String
  subject         Subject      @relation(fields: [subjectId], references: [id])
  
  // Metadata
  type            DocumentType // TEXTBOOK, TEACHER_GUIDE, REFERENCE, EXAM_BANK
  title           String       // "To√°n 6 - T·∫≠p 1"
  author          String?      // "B·ªô GD&ƒêT"
  publisher       String?      // "NXB Gi√°o d·ª•c Vi·ªát Nam"
  publishYear     Int?         // 2024
  edition         String?      // "2024", "Ch√¢n tr·ªùi s√°ng t·∫°o", "K·∫øt n·ªëi tri th·ª©c"
  isbn            String?
  
  // Structure info
  totalChapters   Int?
  totalPages      Int?
  
  // File info
  originalFileName String
  filePath        String?      // S3/local path to original file
  fileSize        Int?
  mimeType        String?
  
  // Processing status
  status          ProcessingStatus @default(PENDING) // PENDING, PROCESSING, COMPLETED, FAILED
  processedAt     DateTime?
  errorMessage    String?
  
  // Relations
  chunks          Chunk[]
  uploadedBy      String?
  uploader        User?        @relation("DocumentUploader", fields: [uploadedBy], references: [id])
  
  createdAt       DateTime     @default(now())
  updatedAt       DateTime     @updatedAt
  
  @@index([subjectId])
  @@index([type])
  @@index([status])
  @@map("documents")
}

enum ProcessingStatus {
  PENDING
  PROCESSING
  COMPLETED
  FAILED
  PARTIALLY_COMPLETED
}

model Chunk {
  id              String       @id @default(cuid())
  documentId      String
  document        Document     @relation(fields: [documentId], references: [id], onDelete: Cascade)
  
  // Location metadata
  chapterNumber   Int?         // 1, 2, 3... (NULL for intro/appendix)
  chapterTitle    String?      // "S·ªë nguy√™n"
  sectionNumber   String?      // "1.1", "2.3"
  sectionTitle    String?      // "T·∫≠p h·ª£p s·ªë nguy√™n"
  pageStart       Int?
  pageEnd         Int?
  
  // Content
  content         String       @db.LongText
  contentLength   Int          // Character count
  tokenCount      Int?         // Estimated tokens
  
  // Embedding (stored locally in MySQL as JSON)
  // For production: use Vector DB (Qdrant/Pinecone) instead
  embedding       Json?        // [float, float, ...] - 3072 dimensions
  embeddingModel  String?      // "text-embedding-3-large"
  
  // Chunk metadata
  chunkIndex      Int          // Order within document
  chunkType       ChunkType    @default(TEXT) // TEXT, TABLE, FORMULA, IMAGE_TEXT
  
  // AI-extracted metadata
  topics          String[]     // ["S·ªë nguy√™n", "Ph√©p c·ªông"]
  keywords        String[]     // Auto-extracted
  difficulty      Difficulty?  // AUTO_DETECTED or MANUAL
  
  // Relations
  questions       Question[]   // Questions generated from this chunk
  
  createdAt       DateTime     @default(now())
  updatedAt       DateTime     @updatedAt
  
  @@index([documentId])
  @@index([chapterNumber])
  @@index([chunkType])
  @@map("chunks")
}

enum ChunkType {
  TEXT           // Normal text content
  TABLE          // Table or structured data
  FORMULA        // Math formula
  IMAGE_TEXT     // OCR text from image
  EXERCISE       // Exercise section
  SUMMARY        // Chapter summary
}

// ===== QUESTIONS =====

model Question {
  id              String       @id @default(cuid())
  subjectId       String
  subject         Subject      @relation(fields: [subjectId], references: [id])
  
  // Source tracking (CRITICAL for zero hallucination)
  sourceChunkId   String?      // Link to source chunk
  sourceChunk     Chunk?       @relation(fields: [sourceChunkId], references: [id])
  sourceDocument  String?      // Document title
  chapterNumber   Int?         // For filtering
  chapterTitle    String?
  pageNumber      Int?
  
  // Content
  type            QuestionType
  content         String       @db.LongText
  options         Json?        // For MCQ: ["A. ...", "B. ...", "C. ...", "D. ..."]
  correctAnswer   String       // "A", "B", "C", "D" or full text
  explanation     String?      @db.LongText
  
  // Metadata
  difficulty      Difficulty   @default(MEDIUM)
  points          Float        @default(1.0)
  timeEstimate    Int?         // Seconds
  bloomLevel      String?      // "Remember", "Understand", "Apply", "Analyze"
  
  // Topics & Keywords
  topics          String[]     // ["Ph∆∞∆°ng tr√¨nh b·∫≠c 2", "C√¥ng th·ª©c nghi·ªám"]
  keywords        String[]
  
  // AI generation metadata
  generatedBy     GenerationMethod @default(AI)
  aiPrompt        String?      // Prompt used to generate
  aiConfidence    Float?       // 0.0 - 1.0
  
  // Status
  status          QuestionStatus @default(DRAFT)
  reviewedBy      String?
  reviewer        User?        @relation("QuestionReviewer", fields: [reviewedBy], references: [id])
  reviewedAt      DateTime?
  
  // Relations
  exams           ExamQuestion[]
  createdBy       String?
  creator         User?        @relation("QuestionCreator", fields: [createdBy], references: [id])
  
  createdAt       DateTime     @default(now())
  updatedAt       DateTime     @updatedAt
  
  @@index([subjectId])
  @@index([chapterNumber])
  @@index([difficulty])
  @@index([status])
  @@index([sourceChunkId])
  @@map("questions")
}

enum GenerationMethod {
  AI              // Generated by AI
  MANUAL          // Created manually by teacher
  IMPORTED        // Imported from external source
  MIXED           // AI-generated but heavily edited
}

enum QuestionStatus {
  DRAFT           // Just created, not reviewed
  REVIEWED        // Teacher reviewed and approved
  PUBLISHED       // Available for use
  ARCHIVED        // Deprecated/old
}

// ===== EXAMS =====

model Exam {
  id              String       @id @default(cuid())
  subjectId       String
  subject         Subject      @relation(fields: [subjectId], references: [id])
  
  title           String       // "Ki·ªÉm tra 15' - Ch∆∞∆°ng 2"
  description     String?
  
  // Scope definition (NEW)
  scope           ExamScope    @default(SINGLE_CHAPTER)
  targetChapters  Int[]        // [2] or [1,2,3] or [] for full
  
  // Distribution (NEW) - How many questions per chapter
  chapterDistribution Json?    // {"1": 5, "2": 10, "3": 5}
  
  // Exam config
  totalPoints     Float
  duration        Int?         // Minutes
  passingScore    Float?
  
  // Difficulty distribution
  difficultyDistribution Json? // {"EASY": 30, "MEDIUM": 50, "HARD": 20}
  
  // Question type distribution
  typeDistribution Json?       // {"MCQ": 60, "SHORT_ANSWER": 30, "ESSAY": 10}
  
  // Status
  status          ExamStatus   @default(DRAFT)
  publishedAt     DateTime?
  
  // Relations
  questions       ExamQuestion[]
  submissions     StudentSubmission[]
  
  createdBy       String?
  creator         User?        @relation("ExamCreator", fields: [createdBy], references: [id])
  
  createdAt       DateTime     @default(now())
  updatedAt       DateTime     @updatedAt
  
  @@index([subjectId])
  @@index([scope])
  @@index([status])
  @@map("exams")
}

enum ExamScope {
  SINGLE_CHAPTER    // Ki·ªÉm tra 1 ch∆∞∆°ng
  MULTI_CHAPTER     // Ki·ªÉm tra nhi·ªÅu ch∆∞∆°ng
  MIDTERM           // Gi·ªØa k·ª≥ (1/2 s√°ch)
  FINAL             // Cu·ªëi k·ª≥ (to√†n b·ªô)
  FULL_BOOK         // √în t·∫≠p to√†n b·ªô
}

enum ExamStatus {
  DRAFT
  PUBLISHED
  ARCHIVED
}

// ===== LESSON PLANS =====

model LessonPlan {
  id              String       @id @default(cuid())
  subjectId       String
  subject         Subject      @relation(fields: [subjectId], references: [id])
  
  // Scope
  chapterNumber   Int
  chapterTitle    String
  lessonNumber    Int?         // Lesson within chapter
  lessonTitle     String
  
  // MOET structure
  duration        Int          // Minutes (usually 45)
  objectives      Json         // {knowledge: [], skills: [], attitude: []}
  teachingMethods Json         // ["Th·∫£o lu·∫≠n nh√≥m", "Gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ"]
  materials       String[]     // ["SGK", "B·∫£ng ph·ª•", "M√°y chi·∫øu"]
  
  // Activities
  warmUp          String?      @db.LongText
  teacherActivities String     @db.LongText
  studentActivities String     @db.LongText
  assessment      Json         // Criteria and methods
  homework        String?      @db.LongText
  
  // Linked resources
  suggestedQuestions String[]  // Question IDs
  sourceChunks    String[]     // Chunk IDs used
  
  // Full content (generated by AI)
  content         String?      @db.LongText
  
  // Status
  status          LessonPlanStatus @default(DRAFT)
  
  createdBy       String?
  creator         User?        @relation("LessonPlanCreator", fields: [createdBy], references: [id])
  
  createdAt       DateTime     @default(now())
  updatedAt       DateTime     @updatedAt
  
  @@index([subjectId])
  @@index([chapterNumber])
  @@index([status])
  @@map("lesson_plans")
}

enum LessonPlanStatus {
  DRAFT
  REVIEWED
  PUBLISHED
  ARCHIVED
}

// ===== AI USAGE TRACKING =====

model AIUsageLog {
  id              String       @id @default(cuid())
  userId          String
  user            User         @relation(fields: [userId], references: [id])
  
  actionType      ActionType
  
  // Context (NEW)
  subjectId       String?
  chapterNumber   Int?         // Track usage per chapter
  scope           String?      // "chapter-2", "full-book"
  
  // Tokens
  tokenUsed       Int
  inputTokens     Int?
  outputTokens    Int?
  
  // Model info
  model           String       @default("gpt-4o")
  cost            Float?       // Estimated cost in USD
  
  // Performance
  duration        Int?         // Milliseconds
  
  // Request/Response
  prompt          String?      @db.LongText
  response        String?      @db.LongText
  
  createdAt       DateTime     @default(now())
  
  @@index([userId])
  @@index([actionType])
  @@index([chapterNumber])
  @@index([createdAt])
  @@map("ai_usage_logs")
}

// ===== PROCESSING JOBS =====

model ProcessingJob {
  id              String       @id @default(cuid())
  type            JobType
  
  // Document reference
  documentId      String?
  userId          String
  
  // Status
  status          JobStatus    @default(QUEUED)
  progress        Int          @default(0) // 0-100
  
  // Timing
  queuedAt        DateTime     @default(now())
  startedAt       DateTime?
  completedAt     DateTime?
  
  // Results
  result          Json?        // Success data
  error           String?      @db.LongText
  
  // Retry
  attempts        Int          @default(0)
  maxAttempts     Int          @default(3)
  
  @@index([userId])
  @@index([status])
  @@index([type])
  @@map("processing_jobs")
}

enum JobType {
  DOCUMENT_UPLOAD
  DOCUMENT_PARSE
  CHUNK_GENERATION
  EMBEDDING_GENERATION
  EXAM_GENERATION
  LESSON_PLAN_GENERATION
  EXPORT_PDF
  EXPORT_WORD
}

enum JobStatus {
  QUEUED
  PROCESSING
  COMPLETED
  FAILED
  CANCELLED
}
```

---

## üêç Phase 2: Python Document Processing Service (Week 2-3)

### Tech Stack:
- **Framework**: FastAPI
- **PDF Parser**: PyMuPDF (fitz), pdfplumber
- **DOCX Parser**: python-docx, mammoth
- **Chunking**: LangChain TextSplitter
- **Embedding**: OpenAI API / sentence-transformers (local)
- **Queue**: Celery + Redis / RabbitMQ

### Service Structure:

```
python-service/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ main.py                 # FastAPI app
‚îÇ   ‚îú‚îÄ‚îÄ config.py               # Settings
‚îÇ   ‚îú‚îÄ‚îÄ celery_app.py           # Celery configuration
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ parsers/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pdf_parser.py       # Extract text, detect structure
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ docx_parser.py      # Parse Word documents
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ excel_parser.py     # Parse Excel
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ structure_detector.py # Detect chapters, sections
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ chunking/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ smart_chunker.py    # Context-aware chunking
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ metadata_extractor.py # Extract topics, keywords
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ embeddings/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ openai_embedder.py  # OpenAI embeddings
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ local_embedder.py   # Local model (optional)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ tasks/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ process_document.py # Celery task
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ generate_embeddings.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ export_document.py
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ vector_db.py        # Qdrant/Pinecone client
‚îÇ       ‚îî‚îÄ‚îÄ database.py         # MySQL connection
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ Dockerfile
```

### Key Functions:

```python
# app/parsers/pdf_parser.py
from typing import List, Dict
import fitz  # PyMuPDF

class PDFParser:
    def parse(self, file_path: str) -> Dict:
        doc = fitz.open(file_path)
        
        chapters = []
        current_chapter = None
        
        for page_num, page in enumerate(doc, start=1):
            text = page.get_text()
            
            # Detect chapter header (regex or ML)
            if self._is_chapter_header(text):
                if current_chapter:
                    chapters.append(current_chapter)
                current_chapter = {
                    'number': len(chapters) + 1,
                    'title': self._extract_chapter_title(text),
                    'start_page': page_num,
                    'content': ''
                }
            
            if current_chapter:
                current_chapter['content'] += text
                current_chapter['end_page'] = page_num
        
        if current_chapter:
            chapters.append(current_chapter)
        
        return {
            'total_pages': len(doc),
            'chapters': chapters,
            'metadata': self._extract_metadata(doc)
        }

# app/chunking/smart_chunker.py
from langchain.text_splitter import RecursiveCharacterTextSplitter

class SmartChunker:
    def __init__(self, chunk_size=1000, chunk_overlap=200):
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
    
    def chunk_chapter(self, chapter: Dict) -> List[Dict]:
        """Chunk a chapter into smaller pieces with metadata"""
        chunks = []
        texts = self.splitter.split_text(chapter['content'])
        
        for i, text in enumerate(texts):
            chunks.append({
                'content': text,
                'chapter_number': chapter['number'],
                'chapter_title': chapter['title'],
                'page_start': chapter['start_page'],
                'page_end': chapter['end_page'],
                'chunk_index': i,
                'topics': self._extract_topics(text),
                'keywords': self._extract_keywords(text)
            })
        
        return chunks

# app/tasks/process_document.py
from celery import Task
from app.celery_app import celery_app

@celery_app.task(bind=True, max_retries=3)
def process_document_task(self: Task, document_id: str, file_path: str):
    try:
        # 1. Parse document
        parser = PDFParser()
        parsed_data = parser.parse(file_path)
        
        # 2. Chunk content
        chunker = SmartChunker()
        all_chunks = []
        for chapter in parsed_data['chapters']:
            chunks = chunker.chunk_chapter(chapter)
            all_chunks.extend(chunks)
        
        # 3. Generate embeddings
        embedder = OpenAIEmbedder()
        for chunk in all_chunks:
            chunk['embedding'] = embedder.embed(chunk['content'])
        
        # 4. Save to database
        database.save_chunks(document_id, all_chunks)
        
        # 5. Update document status
        database.update_document_status(document_id, 'COMPLETED')
        
        return {'status': 'success', 'chunks_count': len(all_chunks)}
        
    except Exception as e:
        # Retry with exponential backoff
        self.retry(countdown=2 ** self.request.retries, exc=e)
```

---

## üîÑ Phase 3: Queue System Integration (Week 3-4)

### NestJS Queue Module:

```bash
npm install @nestjs/bull bull
npm install @nestjs/bullmq bullmq
```

### Implementation:

```typescript
// backend/src/queue/queue.module.ts
import { Module } from '@nestjs/common';
import { BullModule } from '@nestjs/bull';
import { DocumentQueue } from './document.queue';

@Module({
  imports: [
    BullModule.forRoot({
      redis: {
        host: process.env.REDIS_HOST,
        port: parseInt(process.env.REDIS_PORT),
      },
    }),
    BullModule.registerQueue({
      name: 'document-processing',
    }),
  ],
  providers: [DocumentQueue],
  exports: [DocumentQueue],
})
export class QueueModule {}

// backend/src/queue/document.queue.ts
import { Injectable } from '@nestjs/common';
import { InjectQueue } from '@nestjs/bull';
import { Queue } from 'bull';

@Injectable()
export class DocumentQueue {
  constructor(
    @InjectQueue('document-processing') private queue: Queue,
  ) {}

  async addDocumentProcessingJob(documentId: string, filePath: string) {
    return this.queue.add('process-document', {
      documentId,
      filePath,
    }, {
      attempts: 3,
      backoff: {
        type: 'exponential',
        delay: 2000,
      },
    });
  }

  async getJobStatus(jobId: string) {
    const job = await this.queue.getJob(jobId);
    return {
      id: job.id,
      progress: job.progress(),
      state: await job.getState(),
      result: job.returnvalue,
      error: job.failedReason,
    };
  }
}

// backend/src/documents/documents.service.ts (Updated)
async uploadDocument(userId: string, file: Express.Multer.File) {
  // 1. Save file to storage
  const filePath = await this.saveFile(file);
  
  // 2. Create document record
  const document = await this.prisma.document.create({
    data: {
      title: file.originalname,
      originalFileName: file.originalname,
      filePath,
      status: 'PENDING',
      uploadedBy: userId,
    },
  });
  
  // 3. Add to processing queue
  const job = await this.documentQueue.addDocumentProcessingJob(
    document.id,
    filePath,
  );
  
  // 4. Create job tracking record
  await this.prisma.processingJob.create({
    data: {
      id: job.id.toString(),
      type: 'DOCUMENT_UPLOAD',
      documentId: document.id,
      userId,
      status: 'QUEUED',
    },
  });
  
  return {
    documentId: document.id,
    jobId: job.id,
    message: 'Document queued for processing',
  };
}
```

---

## ü§ñ Phase 4: Enhanced AI Orchestrator (Week 4-5)

### Strict RAG with Chapter Filtering:

```typescript
// backend/src/ai/rag.service.ts
@Injectable()
export class RAGService {
  async generateExamQuestions(request: {
    userId: string;
    subjectId: string;
    chapters: number[]; // [2] or [1,2,3] or [] for all
    questionCount: number;
    difficulty: Difficulty;
  }) {
    // 1. Retrieve relevant chunks ONLY from specified chapters
    const chunks = await this.prisma.chunk.findMany({
      where: {
        document: { subjectId: request.subjectId },
        ...(request.chapters.length > 0 && {
          chapterNumber: { in: request.chapters },
        }),
      },
      include: {
        document: true,
      },
    });

    if (chunks.length === 0) {
      throw new BadRequestException(
        `No content found for chapters ${request.chapters.join(', ')}`,
      );
    }

    // 2. Generate query embedding
    const queryText = `Generate ${request.difficulty} level exam questions about chapters ${request.chapters.join(', ')}`;
    const queryEmbedding = await this.aiService.generateEmbedding(queryText);

    // 3. Rank chunks by relevance
    const rankedChunks = await this.rankChunksBySimilarity(
      queryEmbedding,
      chunks,
      20,
    );

    // 4. Build strict context
    const context = rankedChunks.map((chunk, idx) => ({
      content: chunk.content,
      source: `${chunk.document.title} - Ch∆∞∆°ng ${chunk.chapterNumber} - Trang ${chunk.pageStart}-${chunk.pageEnd}`,
      chunkId: chunk.id,
    }));

    // 5. Generate questions with strict prompt
    const systemPrompt = `B·∫°n l√† tr·ª£ l√Ω AI t·∫°o ƒë·ªÅ thi cho gi√°o vi√™n THCS.

QUY T·∫ÆC NGHI√äM NG·∫∂T:
1. CH·ªà s·ª≠ d·ª•ng th√¥ng tin t·ª´ t√†i li·ªáu ngu·ªìn ƒë∆∞·ª£c cung c·∫•p b√™n d∆∞·ªõi.
2. KH√îNG s·ª≠ d·ª•ng ki·∫øn th·ª©c b√™n ngo√†i ho·∫∑c th√¥ng tin chung.
3. M·ªñI c√¢u h·ªèi PH·∫¢I cite ngu·ªìn ch√≠nh x√°c (ch∆∞∆°ng, trang).
4. N·∫øu kh√¥ng ƒë·ªß th√¥ng tin, tr·∫£ v·ªÅ l·ªói r√µ r√†ng.
5. C√¢u h·ªèi ph·∫£i theo ch∆∞∆°ng tr√¨nh THCS Vi·ªát Nam.
6. Format JSON: { "questions": [{ "content", "options", "correctAnswer", "explanation", "source", "chunkId" }] }`;

    const userPrompt = `T·∫°o ${request.questionCount} c√¢u h·ªèi ${request.difficulty} v·ªÅ Ch∆∞∆°ng ${request.chapters.join(', ')}.

T√ÄI LI·ªÜU NGU·ªíN:
${context.map((c, i) => `[${i + 1}] ${c.source}\n${c.content}`).join('\n\n---\n\n')}

Y√™u c·∫ßu:
- ${request.questionCount} c√¢u h·ªèi
- ƒê·ªô kh√≥: ${request.difficulty}
- Ph√¢n b·ªë ƒë·ªÅu trong c√°c ch∆∞∆°ng ƒë∆∞·ª£c ch·ªçn
- M·ªói c√¢u PH·∫¢I c√≥ "source" v√† "chunkId"`;

    const result = await this.aiService.generateStructuredJSON(
      request.userId,
      ActionType.EXAM_GENERATE,
      userPrompt,
      context,
      systemPrompt,
    );

    // 6. Save questions to database
    const questions = await Promise.all(
      result.questions.map((q) =>
        this.prisma.question.create({
          data: {
            subjectId: request.subjectId,
            sourceChunkId: q.chunkId,
            chapterNumber: chunks.find((c) => c.id === q.chunkId)?.chapterNumber,
            content: q.content,
            options: q.options,
            correctAnswer: q.correctAnswer,
            explanation: q.explanation,
            difficulty: request.difficulty,
            status: 'DRAFT',
            generatedBy: 'AI',
            createdBy: request.userId,
          },
        }),
      ),
    );

    return { questions };
  }
}
```

---

## üìÖ Implementation Timeline

### Week 1-2: Database Migration
- [ ] Design new schema
- [ ] Create Prisma migration
- [ ] Data migration script (if needed)
- [ ] Test integrity

### Week 2-3: Python Service
- [ ] Setup FastAPI + Celery
- [ ] Implement PDF/DOCX parsers
- [ ] Smart chunking logic
- [ ] Chapter detection
- [ ] Embedding generation
- [ ] Testing with real textbooks

### Week 3-4: Queue Integration
- [ ] Setup BullMQ + Redis
- [ ] NestJS queue module
- [ ] Job tracking UI
- [ ] Error handling & retry
- [ ] Progress updates

### Week 4-5: Enhanced RAG
- [ ] Chapter-filtered search
- [ ] Strict prompt engineering
- [ ] Source citation
- [ ] Confidence scoring
- [ ] A/B testing

### Week 5-6: Frontend UX
- [ ] Wizard-style generators
- [ ] Job progress tracking
- [ ] Preview & approve flows
- [ ] Export improvements
- [ ] PWA enhancements

### Week 6-7: Testing & Polish
- [ ] End-to-end testing
- [ ] Performance optimization
- [ ] Documentation
- [ ] Deployment

---

## üí∞ Cost Estimation

### Development:
- Backend: 6-7 weeks √ó 40h = 240-280h
- Python Service: 2-3 weeks √ó 40h = 80-120h
- Frontend: 2 weeks √ó 40h = 80h
- Testing: 1 week √ó 40h = 40h
- **Total**: ~440-520h

### Infrastructure (monthly):
- Redis: $10-20 (managed)
- Vector DB (Qdrant Cloud): $50-100
- OpenAI API: ~$100-500 (depends on usage)
- Storage (S3): ~$10-30
- **Total**: ~$170-650/month

---

## üéØ Success Metrics

### Performance:
- Document processing: <2 min for 100-page textbook
- Search latency: <300ms
- Exam generation: <30s for 20 questions

### Quality:
- Question source traceability: 100%
- Teacher approval rate: >80%
- Zero hallucination rate: >95%

### Usage:
- Document upload success rate: >95%
- Job completion rate: >98%
- User satisfaction: >4.5/5

---

## üìù Next Steps

Choose one of these paths:

### Option A: Full Implementation (Recommended for Production)
Implement all phases sequentially for a production-ready system.

### Option B: MVP+ (Quick wins)
Focus on Phase 1 (Database) + Phase 4 (Enhanced RAG) first, defer Python service.

### Option C: Proof of Concept
Build Python service separately, test with sample documents, integrate later.

---

**B·∫°n mu·ªën implement Option n√†o? Ho·∫∑c t√¥i n√™n b·∫Øt ƒë·∫ßu v·ªõi Phase 1 (Database) ngay?**


